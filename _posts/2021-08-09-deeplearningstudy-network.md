---
title:  "[딥러닝스터디] 3. 신경망"
excerpt: "신경망, 활성화함수"
date: 2021-08-07 16:55:00
categories:
  - 딥러닝스터디
  - 딥러닝

tags:
  - 딥러닝스터디
  - 퍼셉트론
---

<br>
<br>

퍼셉트론의 장점은 복잡한 함수를 표현할 수 있지만
**단점**은 가중치를 사람이 수동으로 해야 한다는 것이다. 
이를 해결하는 방법으로 **신경망**이 있다.

신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 있다.

<br>
<br>

# 신경망

신경망은 입력층, 은닉층, 출력층으로 구성된다.
은닉층은 입력층이나 출력층과 달리 사람 눈에 보이지 않는다.

신경망은 모두 3층으로 구성된다. 가중치를 갖는 층은 2개뿐이라 '2층 신경망'이라고도 한다. 문헌에 따라서는 신경망을 구성하는 층수를 기준으로 '3층 신경망'이라고 하는 경우가 있다.

<br>
<br>

# 활성화 함수

앞서 배운 퍼셉트론 식에서 **y = h(b + w1x1 + w2x2)** 다음과 같이 바꿔보았다.
달라진 점은 h(x)라는 함수가 등장했다. 이처럼 입력 신호의 총합을 출력신호로 변환하는 함수를 일반적으로 활성화함수(activation function)이라고 한다.

이러한 활성화함수가 퍼셉트론에서 신경망으로 가기위한 시작이다.

활성화함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수(step function)이라고 한다.
퍼셉트론에서는 활성화 함수로 계단함수를 이용한다.

신경망에서 사용하는 활성화함수는 sigmoid, relu 함수가 있다.

#### 1. 시그모이드 함수 (sigmoid function)

h(x) = 1 / (1 + exp(-x))

위 식이 시그모이드 함수이다.
신경망에서는 활성화 함수로 시그모이드 함수를 이용해 신호를 변환하고, 그 변화된 신호를 다음 뉴런에 전달한다.

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x)) 
```


#### 2. ReLU 함수 (Rectified Linear Unit)

시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나, 최근에는 ReLU 함수를 주로 사용한다.

relu 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.

```python
import numpy as np

def relu(x):
    return np.maximum(0, x) 
```

#### 3. 계단함수 vs sigmoid vs relu

![activationfunction](/img/activationfunction.PNG?raw=true)

계단함수와 시그모이드 함수를 비교해서 계단함수는 0 or 1을 반환하는 반면 시그모이드 함수는 실수를 돌려준다. 즉 연속적인 실수가 신경망에 흐를 수 있다. 시그모이드 함수는 그래프가 매끈하다. 이 매끈함이 신경망 학습에서 아주 중요한 역할을 한다.

두 함수의 공통점이라면 큰관점에서 같은 모양을 하고 있다. 계단함수, 시그모이드 함수 모두 입력이 작을 때의 출력은 0에 가깝고(혹은 0), 입력이 커지면 출력은 1에 가까워진다(혹은1). 즉 두개함수 모두 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. 
입력이 아무리 작거나 커도 출력은 0과 1 사이라는 것도 공통점이다.

계단함수와 시그모이드 함수의 공통점은 직선 한개로 표현 못하는 비선형 함수라는 것이다.

신경망에서는 활성화 함수로 **비선형 함수**를 사용해야 한다. 선형 함수를 사용하면 신경망의 층을 깊게 하는 의미가 없어진다. 그래서 층을 쌓는 해택을 얻고 싶다면 활성화 함수로 반드시 비선형 함수를 사용해야 한다.

<br>
<br>

# 3층 신경망 구현

넘파이 다차원 배열을 사용해 3층 신경망(입력층, 첫번째 은닉층, 두번째 은닉층, 출력층)을 구현할 수 있다.
